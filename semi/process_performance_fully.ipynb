{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.066412Z",
     "start_time": "2023-05-18T12:50:21.142428Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np9\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.139907Z",
     "start_time": "2023-05-18T12:50:22.112423Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eval_dir = 'saved_models/cifar10_uda' #usb_nlp_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_performance_file(fname_neck):\n",
    "    keep_list = []\n",
    "    seed_list = []\n",
    "    event_list = []\n",
    "    # weight_log_list =[]\n",
    "    #                  f'_meta_updating{meta_updating}_w_{w_type}_nor_{normalize_w}'\n",
    "        \n",
    "    for folder_name in os.listdir(eval_dir):\n",
    "        # if folder_name.startswith(fname_head) and (fname_neck in folder_name):\n",
    "        flag = folder_name.endswith('2') or folder_name.endswith('3') or folder_name.endswith('8') \n",
    "        \n",
    "        \n",
    "        if folder_name.startswith(fname_neck) and flag:\n",
    "        # if fname_neck in folder_name:\n",
    "            \n",
    "            event_name = os.listdir(os.path.join(eval_dir,folder_name,'tensorboard'))[-1]\n",
    "            path_event = os.path.join(eval_dir,folder_name,'tensorboard', event_name)\n",
    "            # path_weight = os.path.join(eval_dir,folder_name,'data_weight_log.pt')\n",
    "    \n",
    "            keep_list.append(os.path.join(eval_dir, folder_name, 'log.txt'))\n",
    "            event_list.append(path_event)\n",
    "            # weight_log_list.append(path_weight)\n",
    "            seed_list.append(int(folder_name.split('_')[-1]))\n",
    "    # print(keep_list,len(keep_list))\n",
    "    return seed_list,keep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.140957Z",
     "start_time": "2023-05-18T12:50:22.112423Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "check_metric_list = ['iteration', 'loss', 'top-1-acc']\n",
    "\n",
    "# for avg_method in ['binary', 'macro', 'micro', 'weighted']:\n",
    "#     # for metric in ['precision', 'recall', 'F1']:\n",
    "#     check_metric_list.append(avg_method+\"_\"+metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.148198Z",
     "start_time": "2023-05-18T12:50:22.134005Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_arguments_dict(arguments):\n",
    "    arguments_dict = {}\n",
    "    arguments_=arguments[7:]\n",
    "    for key_value in arguments_[:-1].split(', '):\n",
    "        key, value = key_value.split('=')\n",
    "        value = value.replace(\"'\", \"\")\n",
    "        arguments_dict[key] = value\n",
    "    return arguments_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.321952Z",
     "start_time": "2023-05-18T12:50:22.145904Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_performance_table(fname_neck):\n",
    "    seed_list, keep_list = gather_performance_file(fname_neck)\n",
    "    performance_table_list = []\n",
    "    for seed, file_path in zip(seed_list, keep_list):\n",
    "        with open(file_path, 'r') as file:\n",
    "            # print(seed)\n",
    "            check_metric_dict, arguments_dict, performance_table = None, None, None\n",
    "            drop_it = False\n",
    "            for line in file:\n",
    "    \n",
    "                # new_training_cond = 'Arguments: Namespace'\n",
    "                new_training_cond = 'use_emameta'\n",
    "                check_cond = 'eval/loss'\n",
    "                if new_training_cond in line:\n",
    "                    if (arguments_dict is not None) and (check_metric_dict is not None) and (not drop_it):\n",
    "                        performance_table = pd.DataFrame(check_metric_dict)\n",
    "                        for key, value in arguments_dict.items():\n",
    "                            performance_table[key] = value\n",
    "                        performance_table_list.append(performance_table)\n",
    "                        \n",
    "                    arguments_ = line.split(new_training_cond)[1].replace('(', '{').replace(')', '}').replace(\"\\n\", \"\")\n",
    "                    arguments_dict = get_arguments_dict(arguments_)\n",
    "                check_metric_dict = {key:[] for key in check_metric_list}\n",
    "    #                 print(check_metric_dict)\n",
    "                if check_cond in line:\n",
    "                    for metric in check_metric_dict:\n",
    "                        try:\n",
    "                            if metric == 'iteration':\n",
    "                                metric_value = int(line.split('INFO] ')[1].split(' '+metric)[0])\n",
    "                            else:\n",
    "                            \n",
    "                                metric_value = float(line.split(f\"'eval/{metric}': \")[1].split(',')[0])\n",
    "                            check_metric_dict[metric].append(metric_value)\n",
    "                        except IndexError:\n",
    "                            drop_it = True\n",
    "                            print(f'{file_path} has missing metric')\n",
    "                            break\n",
    "                            \n",
    "                    performance_table = pd.DataFrame(check_metric_dict)\n",
    "                    for key, value in arguments_dict.items():\n",
    "                        performance_table[key] = value\n",
    "                    performance_table_list.append(performance_table)\n",
    "    performance_complete_table = pd.concat(performance_table_list)\n",
    "    return performance_complete_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.336816Z",
     "start_time": "2023-05-18T12:50:22.302657Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def draw_performance(performance_complete_table, group_list, metric_name, filter_cond=None):\n",
    "    if filter_cond is not None:\n",
    "        avg_metric = performance_complete_table.loc[filter_cond].groupby(group_list)[metric_name].mean()\n",
    "    else:\n",
    "        avg_metric = performance_complete_table.groupby(group_list)[metric_name].mean()\n",
    "    plt.plot(avg_metric)\n",
    "    plt.title(metric_name)\n",
    "    plt.show()\n",
    "    # print(avg_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.472573Z",
     "start_time": "2023-05-18T12:50:22.385703Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# filter_cond = performance_complete_table['optim'] == check_optim\n",
    "# draw_performance(performance_complete_table, ['iteration'], 'loss', filter_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T12:50:22.804492Z",
     "start_time": "2023-05-18T12:50:22.803448Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def show_best_mean_std(performance_complete_table, group_list, metric_name, filter_cond=None):\n",
    "    if filter_cond is not None:\n",
    "        best_metric = performance_complete_table.loc[filter_cond].groupby(group_list)[metric_name].max()\n",
    "    else:\n",
    "        best_metric = performance_complete_table.groupby(group_list)[metric_name].max()\n",
    "    # print(best_metric)\n",
    "    best_mean = (best_metric*100).mean()\n",
    "    best_std = (best_metric*100).std()\n",
    "    top_5_mean = (best_metric.sort_values()[-3:]*100).mean()\n",
    "    top_5_std =  (best_metric.sort_values()[-3:]*100).std()\n",
    "    # print(best_metric*100)\n",
    "    return f\"{top_5_mean:.2f} ± {top_5_std:.2f}\"\n",
    "    # print(f\"Best {metric_name} {best_mean:.3f} ± {best_std:.3f}\",'\\n', f\"Top5 {metric_name} {top_5_mean:.3f} ± {top_5_std :.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_cond = performance_complete_table['optim'] == check_optim\n",
    "method_name_l=['fullysupervised_lossnet']\n",
    "# b_fixmatch_lossnet_cifar10_wrn_28_2_4000_lr0.03_True_bsz100_asy_0.4_False_1000_100_mae_uni_0\n",
    "\n",
    "dataset ='cifar10'\n",
    "\n",
    "num_labels = 50000\n",
    "\n",
    "# noise_l = [\"sym_0.2\",'sym_0.5','asy_0.4','human_aggre_0','human_random_0','human_worst_0']\n",
    "noise_l = [\"flip2_0\",'flip2_0.2','flip2_0.4']\n",
    "# noisy_ratio=0.4\n",
    "# noise_l = [\"sym_0\",'sym_0.4','sym_0.6']\n",
    "# net=\"wrn_28_2\"\n",
    "per_dict={key:{} for key in method_name_l}\n",
    "# per_dict={'flexmatch_lossnet':{}}\n",
    "# per_dict={'uda_lossnet':{},'fixmatch_lossnet':{},'flexmatch_lossnet':{}}\n",
    "# per_dict={'fullysupervised_lossnet':{}}\n",
    "# method = method_name_l[3]\n",
    "meta_loss='cer'\n",
    "for method in method_name_l:\n",
    "# for meta_goal in meta_goal_l:\n",
    "    for noise in noise_l:\n",
    "\n",
    "        neck=f\"b_{method}_{dataset}_resnet32_{num_labels}_lr0.1_True_bsz100_{noise}_False_1000_100_{meta_loss}_uni_0.001_beta0\"\n",
    "        \n",
    "        # neck=f\"b_{method}_{dataset}_wrn_28_2_{num_labels}_lr0.03_True_bsz100_{noise}_False_1000_100_mae\"\n",
    "    # b_fullysupervised_lossnet_cifar10_wrn_28_10_50000_lr0.05_True_bsz100_sym_0.4_False_10000_1000_feat_expno1N_uni_5e-06_beta0_13\n",
    "        try:\n",
    "                \n",
    "            performance_complete_table = get_performance_table(neck)\n",
    "            m= show_best_mean_std(performance_complete_table, ['seed'], 'top-1-acc', filter_cond=None)\n",
    "            per_dict[method][noise]=m\n",
    "\n",
    "            # per_dict[noise][meta_goal]=m\n",
    "        except:\n",
    "            per_dict[method][noise]=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flip2_0</th>\n",
       "      <th>flip2_0.2</th>\n",
       "      <th>flip2_0.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fullysupervised_lossnet</th>\n",
       "      <td>92.15 ± 0.06</td>\n",
       "      <td>89.72 ± 0.50</td>\n",
       "      <td>85.43 ± 0.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              flip2_0     flip2_0.2     flip2_0.4\n",
       "fullysupervised_lossnet  92.15 ± 0.06  89.72 ± 0.50  85.43 ± 0.87"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(per_dict).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
