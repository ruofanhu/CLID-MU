{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ec86978e-5d0b-4c4d-b5ec-071b469a17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c650a9e-2b08-421c-a1af-c859de555cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path\n",
    "paths=[\"exp_results\"]\n",
    "\n",
    "# Prefix to filter files\n",
    "goals=['clid','ce','ce_sloss','mae']\n",
    "# Initialize a list to store results\n",
    "datasets=['cifar100_']\n",
    "prefixs=[]\n",
    "for goal in goals:\n",
    "    for dataset in datasets:\n",
    "        prefixs.append(f\"vri_resnet18_{goal}-{dataset}\")\n",
    "results =[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over files in the folder\n",
    "for folder_path in paths:\n",
    "    \n",
    "    for file_prefix in prefixs:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # Check if the file name starts with the desired prefix\n",
    "            if filename.startswith(file_prefix) and 'tau' in filename:\n",
    "                \n",
    "                # Full file path\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                print(filename)\n",
    "                # Extract dataset, corruption_type, and corruption_prob from the filename\n",
    "                _, rest = filename.split(file_prefix, 1)\n",
    "                atts=rest.split(\"_\")\n",
    "                # dataset, corruption_type, corruption_prob= atts[:3]\n",
    "                corruption_type, corruption_prob= atts[:2]\n",
    "                tau=float(atts[-3].split('tau')[1])\n",
    "                best_acc=-1\n",
    "                best_epoch_end_acc_5 = -1\n",
    "                best_epoch_end_acc_1 = -1\n",
    "                best_epoch_accu_5 = -1\n",
    "                best_epoch_accu_1 = -1\n",
    "                best_ema_acc=-1\n",
    "                best_epoch_accu_3=-1\n",
    "                # Read the file and extract the best accuracy\n",
    "                \n",
    "                cond=\"epoch:[150\"\n",
    "                    \n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        if \"epoch_end:\" in line:\n",
    "                            match = re.search(r\"epoch_end:\\s*({.*})\", line)\n",
    "                            if match:\n",
    "                                epoch_end = ast.literal_eval(match.group(1))\n",
    "                                if 'acc_5' in epoch_end:\n",
    "                                    best_epoch_end_acc_5 = epoch_end['acc_5']\n",
    "                                if 'acc_1' in epoch_end:\n",
    "                                    best_epoch_end_acc_1 = epoch_end['acc_1'] \n",
    "                        # Extract the `epoch_accu` dictionary\n",
    "                        elif \"epoch_accu:\" in line:\n",
    "                            match = re.search(r\"epoch_accu:\\s*({.*})\", line)\n",
    "                            if match:\n",
    "                                epoch_accu = ast.literal_eval(match.group(1))\n",
    "                                if 'acc_5' in epoch_accu:\n",
    "                                    best_epoch_accu_5 = epoch_accu['acc_5']\n",
    "                                if 'acc_1' in epoch_accu:\n",
    "                                    best_epoch_accu_1 = epoch_accu['acc_1']\n",
    "                                if 'acc_3' in epoch_accu:\n",
    "                                    best_epoch_accu_3 = epoch_accu['acc_3']       \n",
    "                        if \"test_acc\" in line:\n",
    "                            test_acc_match = re.search(r\"test_acc:([\\d.]+)\", line)\n",
    "                            last_acc = float(test_acc_match.group(1))\n",
    "                            match = re.search(r\"epoch:\\[(\\d+)/(\\d+)\\]\", line)\n",
    "                            if match:\n",
    "                                current_epoch = int(match.group(1))\n",
    "                            if last_acc>best_acc:\n",
    "                                best_acc=last_acc\n",
    "                                best_epoch=current_epoch\n",
    "\n",
    "\n",
    "                        if cond in line:\n",
    "                            results.append({\n",
    "                                \"dataset\": file_prefix,\n",
    "                                \"corruption_type\": corruption_type,\n",
    "                                \"corruption_prob\": float(corruption_prob),\n",
    "                                'tau':tau,\n",
    "                                \"best_acc\": best_acc,\n",
    "                                \"last_acc\": last_acc,\n",
    "                                \"best_epoch\":best_epoch,\n",
    "                                \"best_epoch_accu_5\":best_epoch_accu_5*100,\n",
    "                                \"best_epoch_accu_3\":best_epoch_accu_3*100,\n",
    "                                \"best_epoch_accu_1\":best_epoch_accu_1*100,\n",
    "                                \"best_epoch_acc_5\":best_epoch_end_acc_5*100,\n",
    "                                \"best_epoch_acc_1\":best_epoch_end_acc_1*100,                        \n",
    "                            })\n",
    "                            best_acc=-1\n",
    "                            best_epoch_end_acc_5 = -1\n",
    "                            best_epoch_end_acc_1 = -1\n",
    "                            best_epoch_accu_5 = -1\n",
    "                            best_epoch_accu_1 = -1\n",
    "                            best_ema_acc=-1\n",
    "                            best_epoch_accu_3=-1\n",
    "        \n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Group by (dataset, corruption_type, corruption_prob) and calculate average and std\n",
    "# grouped_results_ = results_df.groupby([\"dataset\", \"corruption_type\", \"corruption_prob\"])[[\"best_acc\",'last_acc',\"best_epoch_acc_5\"]].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Display the grouped results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db29efc-a72d-4c12-b23f-52b4cbe447d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270107/3851985044.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(top3_best_acc)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = (\n",
    "    results_df.groupby([\"dataset\",\"corruption_type\", \"corruption_prob\",\"tau\"], group_keys=False)\n",
    ")\n",
    "grouped_results_ = results.groupby([\"dataset\",\"corruption_type\", \"corruption_prob\",'tau'])[[\"best_acc\",'last_acc',\"best_epoch_acc_5\"]].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "grouped_results_.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col \n",
    "                            for col in grouped_results_.columns]\n",
    "\n",
    "# Combine mean and std into \"mean ± std\" format\n",
    "for col in [\"best_acc\", \"last_acc\", \"best_epoch_acc_5\",]:\n",
    "    grouped_results_[col] = (\n",
    "        grouped_results_[f\"{col}_mean\"].round(2).astype(str) + \n",
    "        \" ± \" +\n",
    "        grouped_results_[f\"{col}_std\"].round(2).astype(str)\n",
    "    )\n",
    "\n",
    "# Drop intermediate mean and std columns\n",
    "grouped_results = grouped_results_.drop(\n",
    "    columns=[f\"{col}_mean\" for col in [\"best_acc\", \"last_acc\", \"best_epoch_acc_5\"]] +\n",
    "            [f\"{col}_std\" for col in [\"best_acc\", \"last_acc\", \"best_epoch_acc_5\"]]\n",
    ")\n",
    "\n",
    "# Transpose the DataFrame for readability\n",
    "grouped_results_T = grouped_results.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6a713-e713-49a7-b1d6-812e99c842cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results_T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
